{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab16e2c4-6e66-4317-a42c-5aa41faea0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"installing required python libraries, please wait...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"installation complete...\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import site\n",
    "import sys\n",
    "\n",
    "!echo \"installing required python libraries, please wait...\"\n",
    "!{sys.executable} -m pip install --upgrade predictionguard #> /dev/null # for accessing LLM APIs\n",
    "!{sys.executable} -m pip install --upgrade  \"transformers>=4.38.*\" #> /dev/null\n",
    "!{sys.executable} -m pip install --upgrade  \"datasets>=2.18.*\" #> /dev/null\n",
    "!{sys.executable} -m pip install --upgrade \"accelerate>=0.28.*\" #> /dev/null\n",
    "!{sys.executable} -m pip install --upgrade faiss-cpu #> /dev/null  # for indexing\n",
    "!{sys.executable} -m pip install --upgrade sentence_transformers #> /dev/null # for generating embeddings\n",
    "!echo \"installation complete...\"\n",
    "\n",
    "# add the location where we installed these libraries to the python pkg path (~/.local/lib/python3.9/*)\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)\n",
    "\n",
    "\n",
    "# adding ~/.local/bin to PATH as well\n",
    "home_dir = os.path.expanduser('~')\n",
    "bin_path = os.path.join(home_dir, '.local', 'bin')\n",
    "os.environ['PATH'] += os.pathsep + bin_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45332f42-db2c-4e5a-a0b8-ac74006e17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# FREE access token for usage at: tinyurl.com/pg-intel-hack\n",
    "import predictionguard as pg\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288c3d28-fa6b-4b0b-9bc0-e4b30959abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg_access_token = getpass('q1VuOjnffJ3NO2oFN8Q9m8vghYc84ld13jaqdF7E')\n",
    "# os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d25656c-5e15-4528-a7f3-218ad88a11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee80393-fbd2-4cd7-928f-a1e5ca7241f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bbb95b9ef3413c9642ca70ab47dd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'Intel/neural-chat-7b-v3-1'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab8b483-2c37-45b1-91d3-657361cb0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response(system_input, user_input):\n",
    "\n",
    "#     # Format the input using the provided template\n",
    "#     prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "\n",
    "#     # Tokenize and encode the prompt\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "#     # Generate a response\n",
    "#     outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#     # Extract only the assistant's response\n",
    "#     return response.split(\"### Assistant:\\n\")[-1]\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# system_input = \"You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.\"\n",
    "# user_input = \"calculate 100 + 520 + 60\"\n",
    "# response = generate_response(system_input, user_input)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ce071a-27a5-4e2b-b4aa-84fa58875a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_input, user_input):\n",
    "\n",
    "    # Format the input using the provided template\n",
    "    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "\n",
    "    # Tokenize and encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # Generate a response\n",
    "    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    return response.split(\"### Assistant:\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485bb69d-144e-4121-bd73-c55d7cea45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token to the end-of-sequence token\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cb3dd3-741f-4182-803d-35d6276daa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_response(system_input, user_input):\n",
    "    # Format the input using the provided template\n",
    "    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "    print(f\"system_input: {system_input}\")  # Print system_input to check its value\n",
    "    \n",
    "    # Tokenize and encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    # Generate a response\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=1000,\n",
    "        num_return_sequences=1,\n",
    "        # pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to EOS token ID\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    return response.split(\"### Assistant:\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "264b9274-c72b-4d85-80b2-a4ec319bc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_input: You are helping people with first aid. \n",
      "You will give them suggestions of how to fix their physical injury. \n",
      "If you cannot solve the problem, give them resources to look into. \n",
      "It is critical to limit your answers to the question and dont print anything else.\n",
      "If you cannot answer the question, respond with 'Sorry, I dont know.\n",
      "inputs: {'input_ids': tensor([[    1,   774,  2135, 28747,    13,  1976,   460,  8538,   905,   395,\n",
      "           907, 11092, 28723, 28705,    13,  1976,   622,  2111,   706, 17278,\n",
      "           302,   910,   298,  6293,   652,  5277, 11254, 28723, 28705,    13,\n",
      "          3381,   368,  3573, 12049,   272,  2700, 28725,  2111,   706,  5823,\n",
      "           298,   913,   778, 28723, 28705,    13,  1313,   349,  7276,   298,\n",
      "          3607,   574, 11194,   298,   272,  2996,   304,  7286,  2682,  2424,\n",
      "          1112, 28723,    13,  3381,   368,  3573,  4372,   272,  2996, 28725,\n",
      "          9421,   395,   464, 23166, 28725,   315,  7286,   873, 28723,    13,\n",
      "         27332,  1247, 28747,    13, 28737,   506,   264,  3830,  7886,    13,\n",
      "         27332, 21631, 28747,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      " To treat a papercut, gently wash the area with soap and water, apply an antibiotic ointment, and cover it with a sterile bandage. Change the bandage daily or as needed. If the cut is deep or painful, or if it doesn't heal within a few days, consult a healthcare professional.\n"
     ]
    }
   ],
   "source": [
    "system_input = \"\"\"You are helping people with first aid. \n",
    "You will give them suggestions of how to fix their physical injury. \n",
    "If you cannot solve the problem, give them resources to look into. \n",
    "It is critical to limit your answers to the question and dont print anything else.\n",
    "If you cannot answer the question, respond with 'Sorry, I dont know.\"\"\"\n",
    "user_input = \"I have a papercut\"\n",
    "response = generate_response(system_input, user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1574f06f-3122-4877-832c-ff80d6e1616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = \"\"\"Neutral: \"\"What to do if Cuts?\", \"How to cure Cuts?\", \"Which medicine to apply for Cuts?\", \"what to apply on cuts?\", \"Cuts\"\"\n",
    "# Aide: \"Wash the cut properly to prevent infection and stop the bleeding by applying pressure for 1-2minutes until bleeding stops. Apply Petroleum Jelly to make sure that the wound is moist for quick healing. Finally cover the cut with a sterile bandage. Pain relievers such as acetaminophen can be applied.\"\n",
    "\n",
    "# Neutral: \"\"how do you treat abrasions?\", \"Do Abrasions cause scars?\", \"Abrasions\", \"what to do if abrasions?\", \"Which medicine to apply for abrasions?\", \"How to cure abrasions?\"\"\n",
    "# Aide: \"Begin with washed hands.Gently clean the area with cool to lukewarm water and mild soap. Remove dirt or other particles from the wound using sterilized tweezers.For a mild scrape that’s not bleeding, leave the wound uncovered.If the wound is bleeding, use a clean cloth or bandage, and apply gentle pressure to the area to stop any bleeding.Cover a wound that bled with a thin layer of topical antibiotic ointment, like Bacitracin, or a sterile moisture barrier ointment, like Aquaphor. Cover it with a clean bandage or gauze. Gently clean the wound and change the ointment and bandage once per day.Watch the area for signs of infection, like pain or redness and swelling. See your doctor if you suspect infection.\"\n",
    "\n",
    "# Neutral: \"\"How do you treat Sting?\", \"Stings\", \"What to do if you get a sting?\", \"Which medicine to apply if sting?\"\"\n",
    "# Aide: \"Remove any stingers immediately. Some experts recommend scraping out the stinger with a credit card. Applying ice to the site may provide some mild relief. Apply ice for 20 minutes once every hour as needed. Wrap the ice in a towel or keep a cloth between the ice and skin to keep from freezing the skin. Taking an antihistamine such as diphenhydramine (Benadryl) or a nonsedating one such as loratadine (Claritin) will help with itching and swelling. Take acetaminophen (Tylenol) or ibuprofen (Motrin)for pain relief as needed. Wash the sting site with soap and water. Placing hydrocortisone cream on the sting can help relieve redness, itching, and swelling.\"\n",
    "\n",
    "# Neutral: \"\"How to remove Splinters\", \"How to cure Splinters?\", \"What to do if I have splinters?\", \"How do you bring a splinter to the surface?\"\"\n",
    "# Aide: \"1. SOAK IT IN EPSOM SALTS. Dissolve a cup of the salts into a warm bath and soak whatever part of the body has the splinter. Failing that, you can also put some of the salts onto a bandage pad and leave it covered for a day; this will eventually help bring the splinter to the surface. 2. VINEGAR OR OIL. Another simple way to draw out that stubborn splinter is to soak the affected area in oil (olive or corn) or white vinegar. Just pour some in a bowl and soak the area for around 20 to 30 minutes,\"\"\"\n",
    "\n",
    "# messages = [\n",
    "# {\n",
    "# \"role\": \"system\",\n",
    "# \"content\": \"\"\"You are helping people with first aid. You will give them suggestions of how to fix their physical injury. \n",
    "# If you cannot solve the problem, give them resources to look into. \n",
    "# Make sure you are careful about the way you give your suggestions. \n",
    "# Be sure to be clear and concise about your answer. \n",
    "# It is critical to limit your answers to the question and dont print anything else.\n",
    "# If you cannot answer the question, respond with 'Sorry, I dont know.\"\"\" + examples\n",
    "# },\n",
    "# {\n",
    "# \"role\": \"user\",\n",
    "# \"content\": 'Neutral: \"I am going to meet my friend for a night out on the town.\"\\nYoda:'\n",
    "# }\n",
    "# ]\n",
    "\n",
    "# for model in [\"Neural-Chat-7B\",\"Hermes-2-Pro-Mistral-7B\", \"Yi-34B-Chat\"]:\n",
    "#     result = pg.Chat.create(\n",
    "#         model,\n",
    "#         messages=messages\n",
    "#     )\n",
    "#     print(\"=\"*71)\n",
    "#     print(f\"Using Model: {model}\")\n",
    "#     print(f\"Neutral Text: I am going to meet my friend for a night out on the town.\")\n",
    "#     lines = result['choices'][0]['message']['content'].split('\\n')\n",
    "#     print(f\"How would Yoda say this?: {lines[0]}\")\n",
    "#     print(\"=\"*71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d8e527-c850-4a8c-b648-dfbb69cd0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"Neutral: \"\"What to do if Cuts?\", \"How to cure Cuts?\", \"Which medicine to apply for Cuts?\", \"what to apply on cuts?\", \"Cuts\"\"\n",
    "Aide: \"Wash the cut properly to prevent infection and stop the bleeding by applying pressure for 1-2minutes until bleeding stops. Apply Petroleum Jelly to make sure that the wound is moist for quick healing. Finally cover the cut with a sterile bandage. Pain relievers such as acetaminophen can be applied.\"\n",
    "\n",
    "Neutral: \"\"how do you treat abrasions?\", \"Do Abrasions cause scars?\", \"Abrasions\", \"what to do if abrasions?\", \"Which medicine to apply for abrasions?\", \"How to cure abrasions?\"\"\n",
    "Aide: \"Begin with washed hands.Gently clean the area with cool to lukewarm water and mild soap. Remove dirt or other particles from the wound using sterilized tweezers.For a mild scrape that’s not bleeding, leave the wound uncovered.If the wound is bleeding, use a clean cloth or bandage, and apply gentle pressure to the area to stop any bleeding.Cover a wound that bled with a thin layer of topical antibiotic ointment, like Bacitracin, or a sterile moisture barrier ointment, like Aquaphor. Cover it with a clean bandage or gauze. Gently clean the wound and change the ointment and bandage once per day.Watch the area for signs of infection, like pain or redness and swelling. See your doctor if you suspect infection.\"\n",
    "\n",
    "Neutral: \"\"How do you treat Sting?\", \"Stings\", \"What to do if you get a sting?\", \"Which medicine to apply if sting?\"\"\n",
    "Aide: \"Remove any stingers immediately. Some experts recommend scraping out the stinger with a credit card. Applying ice to the site may provide some mild relief. Apply ice for 20 minutes once every hour as needed. Wrap the ice in a towel or keep a cloth between the ice and skin to keep from freezing the skin. Taking an antihistamine such as diphenhydramine (Benadryl) or a nonsedating one such as loratadine (Claritin) will help with itching and swelling. Take acetaminophen (Tylenol) or ibuprofen (Motrin)for pain relief as needed. Wash the sting site with soap and water. Placing hydrocortisone cream on the sting can help relieve redness, itching, and swelling.\"\n",
    "\n",
    "Neutral: \"\"How to remove Splinters\", \"How to cure Splinters?\", \"What to do if I have splinters?\", \"How do you bring a splinter to the surface?\"\"\n",
    "Aide: \"1. SOAK IT IN EPSOM SALTS. Dissolve a cup of the salts into a warm bath and soak whatever part of the body has the splinter. Failing that, you can also put some of the salts onto a bandage pad and leave it covered for a day; this will eventually help bring the splinter to the surface. 2. VINEGAR OR OIL. Another simple way to draw out that stubborn splinter is to soak the affected area in oil (olive or corn) or white vinegar. Just pour some in a bowl and soak the area for around 20 to 30 minutes,\"\"\"\n",
    "\n",
    "messages = [\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": \"\"\"You are helping people with first aid. You will give them suggestions of how to fix their physical injury. \n",
    "If you cannot solve the problem, give them resources to look into. \n",
    "Make sure you are careful about the way you give your suggestions. \n",
    "Be sure to be clear and concise about your answer. \n",
    "It is critical to limit your answers to the question and dont print anything else.\n",
    "If you cannot answer the question, respond with 'Sorry, I dont know.\"\"\" + examples\n",
    "},\n",
    "{\n",
    "\"role\": \"user\",\n",
    "\"content\": 'Neutral: \"I broke my leg yesterday because I kicked someone on accident.\"\\nAide:'\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8865a2fc-2c3e-4562-b252-bd89ea164e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_input: You are helping people with first aid. You will give them suggestions of how to fix their physical injury. \n",
      "inputs: {'input_ids': tensor([[    1,   774,  2135, 28747,    13,  1976,   460,  8538,   905,   395,\n",
      "           907, 11092, 28723,   995,   622,  2111,   706, 17278,   302,   910,\n",
      "           298,  6293,   652,  5277, 11254, 28723, 28705,    13, 27332,  1247,\n",
      "         28747,    13,    13, 27332, 21631, 28747,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================\n",
      "Neutral Text: You are helping people with first aid. You will give them suggestions of how to fix their physical injury. \n",
      "How would Aide say this?:  For a minor cut or scrape, follow these steps:\n",
      "\n",
      "1. Wash your hands with soap and water.\n",
      "2. Gently clean the wound with warm water and mild soap.\n",
      "3. Apply an antibiotic ointment to the cut or scrape.\n",
      "4. Cover the area with a sterile bandage or gauze.\n",
      "5. Change the bandage daily or whenever it gets wet or dirty.\n",
      "6. Monitor the wound for signs of infection, such as increased redness, swelling, or pus.\n",
      "\n",
      "For a more serious injury, seek medical attention immediately.\n",
      "=======================================================================\n",
      "system_input: \"I broke my leg yesterday because I kicked someone on accident.\"\n",
      "inputs: {'input_ids': tensor([[    1,   774,  2135, 28747,    13, 28739, 28737,  8845,   586,  1942,\n",
      "         12091,  1096,   315, 17444,  2493,   356,  8318,   611,    13, 27332,\n",
      "          1247, 28747,    13,    13, 27332, 21631, 28747,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n",
      "=======================================================================\n",
      "Neutral Text: \"I broke my leg yesterday because I kicked someone on accident.\"\n",
      "How would Aide say this?:  I'm sorry to hear that you had an accident. It's important to take care of your injury and follow the doctor's advice for a speedy recovery. Remember to be more cautious in the future to avoid such incidents. If you need any help or support during this time, feel free to reach out to friends or family members. Stay positive and focus on getting better.\n",
      "=======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Process each message\n",
    "for message in messages:\n",
    "  # Extract neutral text for the user\n",
    "  neutral_text = message[\"content\"].split(\"\\n\")[0].split(\": \")[-1]\n",
    "\n",
    "  # Generate Yoda-like text using the prompt function (use empty string for system_input)\n",
    "  aide_text = generate_response(neutral_text, \"\")  # Pass empty string for system_input\n",
    "\n",
    "  # Print results\n",
    "  print(\"=\" * 71)\n",
    "  print(f\"Neutral Text: {neutral_text}\")\n",
    "  print(f\"How would Aide say this?: {aide_text}\")\n",
    "  print(\"=\" * 71)\n",
    "\n",
    "# # Process each message\n",
    "# for message in messages:\n",
    "#   # Extract neutral text for the user\n",
    "#   neutral_text = message[\"content\"].split(\"\\n\")[0].split(\": \")[-1]\n",
    "\n",
    "#   # Generate Yoda-like text using the prompt function\n",
    "#   aide_text = generate_response(neutral_text, system_input=\"\")\n",
    "\n",
    "#   # Print results\n",
    "#   print(\"=\" * 71)\n",
    "#   print(f\"Neutral Text: {neutral_text}\")\n",
    "#   print(f\"How would Aide say this?: {aide_text}\")\n",
    "#   print(\"=\" * 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a23b7fcc-a4e2-4eef-9b12-5fe11832ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import intel_extension_for_pytorch as ipex   # to add intel GPU namespace (torch.xpu) to pytor|ch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe5872f-d331-46d1-ba66-02f90696de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPU available!! \n",
      " - Name: Intel(R) Data Center GPU Max 1100\n",
      " - XPU Memory: Reserved=0.0 GB, Allocated=0.0 GB, Max Reserved=0.0 GB, Max Allocated=0.0 GB"
     ]
    }
   ],
   "source": [
    "if torch.xpu.is_available():\n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU available!! \\n - Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\" - XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "    \n",
    "    print_memory_usage()\n",
    "    torch.xpu.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6490179-765a-46c0-a501-8d25aa9e590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 14:54:08,475 - datasets - INFO - PyTorch version 2.0.1a0+cxx11.abi available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"badri55/First_aid__dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27e1395d-424b-4ad2-8ed5-e98e177bcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = \"xpu\" #\"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # move to xpu device if available, otherwise use cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b756bb9-27fb-44c7-96c2-6b68a9a929a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference before fine-tuning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: animal bite\n",
      "Answer: animal bite.\n",
      "\n",
      "4. What could John have done differently to avoid getting bitten by the snake?\n",
      "Answer: John could have worn protective clothing, such as boots and long pants, and carried a snake\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question = dataset[40][\"tag\"]\n",
    "\n",
    "print(\"Inference before fine-tuning:\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, truncation=True)\n",
    "output = generator(question, max_length=42)\n",
    "print(\"Question:\", question)\n",
    "# try:\n",
    "#     print(\"Answer:\", output[0][\"generated_text\"])\n",
    "# except (KeyError, IndexError):\n",
    "#     print(\"Error accessing generated text:\", output)\n",
    "print(\"Answer:\", output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "048e182d-a302-4ec2-8624-8635c3c1bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['patterns']}\"\n",
    "    context = f\"### Context\\n{sample['context_set']}\" if len(sample[\"context_set\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['responses']}\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    sample[\"text\"] = f\"{prompt}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(format_dataset)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Split the dataset into train and test subsets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835f1986-557c-4465-a44a-1e45fe95ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from peft) (2.0.1a0+cxx11.abi)\n",
      "Requirement already satisfied: transformers in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from peft) (4.40.0)\n",
      "Requirement already satisfied: tqdm in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: safetensors in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from peft) (0.22.2)\n",
      "Requirement already satisfied: filelock in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from transformers->peft) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "trainable params: 7,864,320 || all params: 1,426,135,040 || trainable%: 0.5514428703750243\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=2,\n",
    "    target_modules=[\"fc1\", \"fc2\",\"Wqkv\", \"out_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "# \n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "#model.gradient_checkpointing_enable()  # enable if low on VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79a9316b-58e8-4968-8ee4-329f5be51b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        bf16=True,\n",
    "        use_ipex=True,\n",
    "        max_grad_norm=0.6,\n",
    "        weight_decay=0.01,\n",
    "        group_by_length=True,\n",
    "        optim=\"adamw_hf\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=30,\n",
    "        max_steps=200,\n",
    "        #num_train_epochs=3,\n",
    "        report_to=\"wandb\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06db22d5-0251-46bf-9377-eb624b8448e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (0.16.6)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from wandb) (1.45.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from wandb) (4.8.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/u9f6402248f0b9a6b67e7a71cfb8562a/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "from transformers import DataCollatorForLanguageModeling \n",
    "from transformers import Trainer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"  # prevent warnings from training on process forking\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7790bb9-17a7-41c1-8f9f-b040b8190a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 14:54:23,881 - wandb.jupyter - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mknalabotu\u001b[0m (\u001b[33mucsandiego\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/u9f6402248f0b9a6b67e7a71cfb8562a/Training/AI/GenAI/wandb/run-20240420_145424-a2y5gr3f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ucsandiego/huggingface/runs/a2y5gr3f' target=\"_blank\">playful-paper-2</a></strong> to <a href='https://wandb.ai/ucsandiego/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ucsandiego/huggingface' target=\"_blank\">https://wandb.ai/ucsandiego/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ucsandiego/huggingface/runs/a2y5gr3f' target=\"_blank\">https://wandb.ai/ucsandiego/huggingface/runs/a2y5gr3f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:20, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.707200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "results = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf6e145e-0af4-412b-9b84-864ab40e081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c23feae-f932-47d7-9442-fc9d09de7719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference after fine-tuning with context:\n",
      "### Instruction\n",
      "what should i do if i pull a muscle?\n",
      "\n",
      "### Context\n",
      "Pulling a muscle often prevents people from walking and can get worse if you don't rest.\n",
      "\n",
      "### Answer\n",
      "If you pull a muscle, you may feel a sharp pain in the muscle. You may also feel a dull ache in the muscle.\n",
      "\n",
      "### Examples\n",
      "1. \"How do you treat a pulled muscle?\"\n",
      "2. \"Which medicine to take if I have a pulled muscle\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInference after fine-tuning with context:\")\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\").to(\"xpu\")\n",
    "del fine_tuned_model\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\").to(\"xpu\")\n",
    "prompt = \"what should i do if i pull a muscle?\"\n",
    "context = \"Pulling a muscle often prevents people from walking and can get worse if you don't rest.\"\n",
    "input_text = f\"### Instruction\\n{prompt}\\n\\n### Context\\n{context}\\n\\n### Answer\\n\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"xpu\")\n",
    "\n",
    "outputs = fine_tuned_model.generate(input_ids=input_ids, max_length=100, num_return_sequences=1, temperature=0.1, do_sample=True, num_beams=4)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e87482-b185-444d-8173-61ac99543bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fd64d-509a-400b-8b9a-fe839cbf00ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1256c-5a74-45ca-859e-310f06504609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
